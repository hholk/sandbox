model,company,elo,price,context,weight,reasoning,features,benchmark,benchmarkText,source
Gemini 2.5 Pro,Google,1471,0.40,1000000,closed,true,"image;speech",MMLU,"Massive Multitask Language Understanding (57 subjects) exam – Gemini Ultra scores 90.0%, becoming the first model to exceed human expert level on this benchmark ahead of GPT-4’s 86.4%",https://blog.google
Gemini 2.5 Pro,Google,1471,0.40,1000000,closed,true,"image;speech",Code Generation,"Programming challenges – Gemini Ultra outperforms GPT-4 in code tasks, e.g. solving 74.4% of HumanEval coding problems vs GPT-4’s 67.0% (and 74.9% vs 73.9% on a more complex coding test Natural2Code)",https://addepto.com
GPT-4 (2025),OpenAI,1429,60,128000,closed,false,"image;speech",HellaSwag,"Commonsense reasoning benchmark – GPT-4 leads with 95.3% accuracy on HellaSwag, significantly above Gemini’s 87.8%. This test requires choosing plausible endings to everyday scenarios, where GPT-4’s nuanced understanding of context gives it an edge",https://addepto.com
GPT-4 (2025),OpenAI,1429,60,128000,closed,false,"image;speech",ARC-AGI,"ARC-AGI v2 (Advanced Reasoning Challenge) – an extremely difficult pattern recognition and problem-solving test. GPT-4 achieves single-digit success (most models <5%), far behind Grok 4’s leading 15.9%. This underscores the gap that emerging frontier models are closing on GPT-4",https://introl.com
Claude Opus 4,Anthropic,1405,75,200000,closed,false,"",MMLU,"Knowledge and reasoning exam – Claude 4 (Opus) scores ~87% on MMLU, roughly on par with GPT-4. This broad 57-topic test highlights Claude’s general competency, though it slightly trails Gemini Ultra’s 90% top score",https://openlm.ai
Claude Opus 4,Anthropic,1405,75,200000,closed,false,"",AIME 2025,"Math competition (American Invitational Math Exam) – Claude 4 solved about 75.5% of problems, but was outperformed by Gemini (88.0%) and xAI’s Grok 4 (91.7% without tools, 100% with tools). This suggests Claude is strong in math but still lags the best models on challenging competition-level questions",https://introl.com
Llama 4 Maverick,Meta,1410,0,4096,open,false,"image;speech",Chatbot Arena (Elo),"Crowdsourced head-to-head chatbot comparison – Llama 4 Maverick achieved ~1410 Elo, ranking #2 globally in LMSYS Chatbot Arena. This indicates GPT-4-level conversational performance as judged by human voters, although official benchmark scores (MMLU, GSM8K, etc.) are not yet published",https://medium.com
Llama 4 Maverick,Meta,1410,0,4096,open,false,"image;speech",Multimodal,"Multimodal tasks – Llama 4 is natively multimodal, able to interpret images and text simultaneously. For example, the 17B MoE variant Llama 4 Maverick 17B-128E can analyze images with detailed understanding, a capability that previous Llama models lacked. (Full benchmark results pending official release.)",https://ai.meta.com
Grok 4,xAI,1436,0,256000,closed,true,"image;speech",Humanity’s Last Exam,"Open-ended extremely difficult exam – Grok 4 scored 25.4% (text-only) on “Humanity’s Last Exam,” vastly outperforming other models (which hovered ~10–12%). With tool-use enabled (Grok 4 Heavy mode), it reached 44.4%, nearly doubling Google’s and OpenAI’s best (~21–25%). This showcases Grok’s advanced reasoning abilities on the hardest domain questions",https://dig.watch
Grok 4,xAI,1436,0,256000,closed,true,"image;speech",ARC-AGI,"Advanced reasoning test by ARC – Grok 4 achieved 16.2% on the ARC-AGI 2 challenge, nearly double the next-best model (Anthropic’s Claude ~8.6%). Most models struggle to get even 5%. Grok’s top performance on this test of fluid intelligence underscores its frontier status in reasoning",https://dig.watch
Mistral Medium 3,Mistral AI,1370,2,131072,closed,false,"image",SWE-Bench,"Software Engineering Benchmark (agentic coding) – Mistral’s 24B model excels in coding tasks. DevStral Medium (a Mistral fine-tune) scores 61.6% on SWE-Bench (validated), outperforming GPT-4.1 and even Google’s Gemini 2.5 Pro on complex code generation. This demonstrates Mistral models’ high coding proficiency at a fraction of the scale",https://openrouter.ai
Mistral Medium 3,Mistral AI,1370,2,131072,closed,false,"image",Multimodal Reasoning,"Visual and textual reasoning – Mistral Medium 3 is optimized for multimodal inputs, with strong results on tasks like chart interpretation (ChartQA) and document visual QA (DocVQA). It balances state-of-the-art reasoning and vision performance at 8× lower operational cost than larger closed models, making it suitable for scalable deployment",https://openrouter.ai
DeepSeek R1 (2025),DeepSeek,1425,2.19,64000,open,true,"",Math Benchmarks,"Academic math challenges – DeepSeek V3 (685B MoE) reportedly surpassed OpenAI’s GPT-4 (o1) on the MATH dataset and the AIME competition. Its distilled versions (e.g. DeepSeek-R1) use chain-of-thought reasoning and achieve high accuracy with far less compute, disrupting expectations in math problem-solving",https://en.wikipedia.org
DeepSeek R1 (2025),DeepSeek,1425,2.19,64000,open,true,"",Open-Weight,"Model availability – DeepSeek’s models are released as “open weight,” i.e. the exact model parameters are openly shared. This open-source approach (MIT License) combined with cutting-edge performance (on par with GPT-4 in many tasks) has sent shockwaves through the industry, proving that smaller players can challenge tech giants in AI",https://en.wikipedia.org
Qwen 3 235B,Alibaba Cloud,1426,0,131072,open,false,"",MMLU,"General knowledge test – Qwen 3 (Alibaba) scores ~84% on MMLU (with chain-of-thought “thinking” mode). This approaches GPT-4’s performance on the 57-topic exam. Qwen models (7B to 235B parameters) are open-source and demonstrate strong multilingual understanding, closing the gap with Western models on benchmarks like MMLU",https://openlm.ai
Qwen 3 235B,Alibaba Cloud,1426,0,131072,open,false,"",Vision,"Vision-Language capability – Alibaba’s Qwen lineup includes Qwen-VL models that handle image+text inputs. For example, Qwen-VL-32B can answer visual questions with 77.8% accuracy on VQAv2, comparable to GPT-4V. This multimodal proficiency extends Qwen’s utility beyond text, enabling complex tasks like document understanding (e.g. scoring 90.9% on DocVQA)",https://addepto.com
